{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS baseline RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wNykS9FUG3QQ",
        "3zsEP0AsXhW1"
      ],
      "authorship_tag": "ABX9TyMPLfsxR8ntVZrdx++MYg1T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minshyee/NLP_project/blob/baseline_song/STS_baseline_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer library\n",
        "!pip install transformers==3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwaICphKVEj8",
        "outputId": "77803dd2-736a-4848-aa99-e2f90349d034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (21.3)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.96)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.53)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhVD4mCFSoV6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tarfile\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader,  RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_drOUCi7XeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQGP6C1eUnL5",
        "outputId": "022e829f-3fcf-486f-8dc8-f466a5ec60d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 경로설정"
      ],
      "metadata": {
        "id": "d-9HWLF4Wkxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gcYha4KWdl8",
        "outputId": "c19b2945-0c9f-41f1-a95c-f30d42afcd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/AIBootCamp/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljDplJaWWnq9",
        "outputId": "3d115abe-0863-4974-f35c-dfc71f881ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AIBootCamp/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")\n",
        "_DATA_DIR = os.path.join(_CUR_DIR, \"data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-_bpuwWYDnb",
        "outputId": "53e7de21-55fa-4c3d-efab-099c6677da4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My current directory : /content/drive/MyDrive/AIBootCamp/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data download"
      ],
      "metadata": {
        "id": "wNykS9FUG3QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eegs8aWXFcK",
        "outputId": "15d7394c-45c0-4b98-81bd-f102f9e9af54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 22:38:56--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.218.216.130\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.218.216.130|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  1.29MB/s    in 1.0s    \n",
            "\n",
            "2022-05-25 22:38:58 (1.29 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 압출 풀기\n",
        "tar_bz2_file = tarfile.open(_CUR_DIR + \"/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=_DATA_DIR)\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "nmcBR8-hX6C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data load"
      ],
      "metadata": {
        "id": "GxjhDk56G78v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_all = pd.read_json('./data/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "test = pd.read_json('./data/klue-sts-v1.1/klue-sts-v1.1_dev.json')"
      ],
      "metadata": {
        "id": "g9Hq_f6jYf0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_all.sample(frac=1, random_state = seed, replace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "z3JChfHBSJvp",
        "outputId": "bda34622-214e-4fbe-eb71-9d949b82f4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          guid           source  \\\n",
              "7270   klue-sts-v1_train_07270     paraKQC-para   \n",
              "860    klue-sts-v1_train_00860       airbnb-rtt   \n",
              "5390   klue-sts-v1_train_05390   airbnb-sampled   \n",
              "5191   klue-sts-v1_train_05191       policy-rtt   \n",
              "11284  klue-sts-v1_train_11284       policy-rtt   \n",
              "...                        ...              ...   \n",
              "1660   klue-sts-v1_train_01660   airbnb-sampled   \n",
              "5692   klue-sts-v1_train_05692  paraKQC-sampled   \n",
              "2354   klue-sts-v1_train_02354     paraKQC-para   \n",
              "8457   klue-sts-v1_train_08457   airbnb-sampled   \n",
              "6461   klue-sts-v1_train_06461   airbnb-sampled   \n",
              "\n",
              "                                          sentence1  \\\n",
              "7270         네이버 블로그 내용을 백업없이 두지 마세요. 꼭 컴퓨터에 백업하세요.   \n",
              "860                    위치도 시설도 호스트분도 부족함 없이 완벽했습니다.   \n",
              "5390                  하나 단점이라면 방에서의 와이파이가 좀 약한편입니다.   \n",
              "5191               정부는 우리 경제와 산업, 민생을 반드시 지켜낼 것입니다.   \n",
              "11284  한편으로는 병사 급여 인상 등 장병 처우 개선에도 3조8천억 원을 반영했습니다.   \n",
              "...                                             ...   \n",
              "1660                  저는 빈에 온다면 꼭 다시 그녀의 숙소를 이용할거예요   \n",
              "5692               조명등 밝기는 높게 말고 낮게 조정해서 써주시기 바랍니다.   \n",
              "2354                         중국에서 지메일을 사용하는 방법을 알려줘   \n",
              "8457                          기본적인 요리도구와 향신료가 있습니다.   \n",
              "6461    어른 넷이서 자긴 힘들고 어른 세명, 아이가 있음 네명까지 취침이 가능합니다.   \n",
              "\n",
              "                                 sentence2  \\\n",
              "7270    반드시 네이버 블로그 내용을 노트북 대신 컴퓨터에 백업하시오.   \n",
              "860                 위치, 시설, 호스트 모두 완벽했습니다.   \n",
              "5390         완전 도심지는 아닌데 교통이 굉장히 편리한 편입니다.   \n",
              "5191     정부는 우리의 경제, 산업, 그리고 민생을 보호할 것입니다.   \n",
              "11284  반면, 3조 8천억 원은 군인들의 처우 개선에도 반영되었습니다.   \n",
              "...                                    ...   \n",
              "1660          밀라노에 다시온다면 이 숙소를 또 이용할 것입니다.   \n",
              "5692             너희들이 좋아하는 클래식 곡이 뭔지 알고싶구나   \n",
              "2354                지메일을 중국에서 사용하는 방법을 알려줘   \n",
              "8457               감각적인 인테리어가 돋보이던 숙소였습니다.   \n",
              "6461            소파베드를 사용하면 5명까지 이용이 가능합니다.   \n",
              "\n",
              "                                                  labels  \\\n",
              "7270   {'label': 3.5, 'real-label': 3.5, 'binary-labe...   \n",
              "860    {'label': 4.1, 'real-label': 4.142857142857143...   \n",
              "5390   {'label': 0.0, 'real-label': 0.0, 'binary-labe...   \n",
              "5191   {'label': 4.1, 'real-label': 4.142857142857143...   \n",
              "11284  {'label': 3.6, 'real-label': 3.571428571428572...   \n",
              "...                                                  ...   \n",
              "1660   {'label': 2.0, 'real-label': 2.0, 'binary-labe...   \n",
              "5692   {'label': 0.0, 'real-label': 0.0, 'binary-labe...   \n",
              "2354   {'label': 4.5, 'real-label': 4.5, 'binary-labe...   \n",
              "8457   {'label': 0.0, 'real-label': 0.0, 'binary-labe...   \n",
              "6461   {'label': 2.0, 'real-label': 2.0, 'binary-labe...   \n",
              "\n",
              "                                             annotations  \n",
              "7270   {'agreement': '0:0:1:1:4:0', 'annotators': ['0...  \n",
              "860    {'agreement': '0:0:0:2:2:3', 'annotators': ['0...  \n",
              "5390   {'agreement': '6:0:0:0:0:0', 'annotators': ['0...  \n",
              "5191   {'agreement': '0:0:0:2:2:3', 'annotators': ['1...  \n",
              "11284  {'agreement': '0:0:0:3:4:0', 'annotators': ['0...  \n",
              "...                                                  ...  \n",
              "1660   {'agreement': '0:1:4:1:0:0', 'annotators': ['0...  \n",
              "5692   {'agreement': '7:0:0:0:0:0', 'annotators': ['1...  \n",
              "2354   {'agreement': '0:0:0:1:1:4', 'annotators': ['1...  \n",
              "8457   {'agreement': '6:0:0:0:0:0', 'annotators': ['0...  \n",
              "6461   {'agreement': '0:1:3:1:0:0', 'annotators': ['0...  \n",
              "\n",
              "[11668 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a9516bb-3bb1-43c5-82d2-b18c535338f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>guid</th>\n",
              "      <th>source</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>labels</th>\n",
              "      <th>annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>klue-sts-v1_train_07270</td>\n",
              "      <td>paraKQC-para</td>\n",
              "      <td>네이버 블로그 내용을 백업없이 두지 마세요. 꼭 컴퓨터에 백업하세요.</td>\n",
              "      <td>반드시 네이버 블로그 내용을 노트북 대신 컴퓨터에 백업하시오.</td>\n",
              "      <td>{'label': 3.5, 'real-label': 3.5, 'binary-labe...</td>\n",
              "      <td>{'agreement': '0:0:1:1:4:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>klue-sts-v1_train_00860</td>\n",
              "      <td>airbnb-rtt</td>\n",
              "      <td>위치도 시설도 호스트분도 부족함 없이 완벽했습니다.</td>\n",
              "      <td>위치, 시설, 호스트 모두 완벽했습니다.</td>\n",
              "      <td>{'label': 4.1, 'real-label': 4.142857142857143...</td>\n",
              "      <td>{'agreement': '0:0:0:2:2:3', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>klue-sts-v1_train_05390</td>\n",
              "      <td>airbnb-sampled</td>\n",
              "      <td>하나 단점이라면 방에서의 와이파이가 좀 약한편입니다.</td>\n",
              "      <td>완전 도심지는 아닌데 교통이 굉장히 편리한 편입니다.</td>\n",
              "      <td>{'label': 0.0, 'real-label': 0.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '6:0:0:0:0:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>klue-sts-v1_train_05191</td>\n",
              "      <td>policy-rtt</td>\n",
              "      <td>정부는 우리 경제와 산업, 민생을 반드시 지켜낼 것입니다.</td>\n",
              "      <td>정부는 우리의 경제, 산업, 그리고 민생을 보호할 것입니다.</td>\n",
              "      <td>{'label': 4.1, 'real-label': 4.142857142857143...</td>\n",
              "      <td>{'agreement': '0:0:0:2:2:3', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11284</th>\n",
              "      <td>klue-sts-v1_train_11284</td>\n",
              "      <td>policy-rtt</td>\n",
              "      <td>한편으로는 병사 급여 인상 등 장병 처우 개선에도 3조8천억 원을 반영했습니다.</td>\n",
              "      <td>반면, 3조 8천억 원은 군인들의 처우 개선에도 반영되었습니다.</td>\n",
              "      <td>{'label': 3.6, 'real-label': 3.571428571428572...</td>\n",
              "      <td>{'agreement': '0:0:0:3:4:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>klue-sts-v1_train_01660</td>\n",
              "      <td>airbnb-sampled</td>\n",
              "      <td>저는 빈에 온다면 꼭 다시 그녀의 숙소를 이용할거예요</td>\n",
              "      <td>밀라노에 다시온다면 이 숙소를 또 이용할 것입니다.</td>\n",
              "      <td>{'label': 2.0, 'real-label': 2.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '0:1:4:1:0:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5692</th>\n",
              "      <td>klue-sts-v1_train_05692</td>\n",
              "      <td>paraKQC-sampled</td>\n",
              "      <td>조명등 밝기는 높게 말고 낮게 조정해서 써주시기 바랍니다.</td>\n",
              "      <td>너희들이 좋아하는 클래식 곡이 뭔지 알고싶구나</td>\n",
              "      <td>{'label': 0.0, 'real-label': 0.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '7:0:0:0:0:0', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2354</th>\n",
              "      <td>klue-sts-v1_train_02354</td>\n",
              "      <td>paraKQC-para</td>\n",
              "      <td>중국에서 지메일을 사용하는 방법을 알려줘</td>\n",
              "      <td>지메일을 중국에서 사용하는 방법을 알려줘</td>\n",
              "      <td>{'label': 4.5, 'real-label': 4.5, 'binary-labe...</td>\n",
              "      <td>{'agreement': '0:0:0:1:1:4', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8457</th>\n",
              "      <td>klue-sts-v1_train_08457</td>\n",
              "      <td>airbnb-sampled</td>\n",
              "      <td>기본적인 요리도구와 향신료가 있습니다.</td>\n",
              "      <td>감각적인 인테리어가 돋보이던 숙소였습니다.</td>\n",
              "      <td>{'label': 0.0, 'real-label': 0.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '6:0:0:0:0:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6461</th>\n",
              "      <td>klue-sts-v1_train_06461</td>\n",
              "      <td>airbnb-sampled</td>\n",
              "      <td>어른 넷이서 자긴 힘들고 어른 세명, 아이가 있음 네명까지 취침이 가능합니다.</td>\n",
              "      <td>소파베드를 사용하면 5명까지 이용이 가능합니다.</td>\n",
              "      <td>{'label': 2.0, 'real-label': 2.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '0:1:3:1:0:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11668 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a9516bb-3bb1-43c5-82d2-b18c535338f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a9516bb-3bb1-43c5-82d2-b18c535338f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a9516bb-3bb1-43c5-82d2-b18c535338f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_length = int(len(train_all)*0.9)\n",
        "train = train_all[:train_length]\n",
        "valid = train_all[train_length:]"
      ],
      "metadata": {
        "id": "83F9gaGfbLY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, valid.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBUFfJnXYyjR",
        "outputId": "23b3d017-8f9e-4553-ea43-8dbcdba0ac42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10501, 6), (1167, 6), (519, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET"
      ],
      "metadata": {
        "id": "bPtAaOaEZDYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_input_ex_sts(dataset):\n",
        "  '''\n",
        "  transform to the Input example\n",
        "  '''\n",
        "  sent_pairs = [sent1+' '+sent2 for sent1, sent2 in zip(dataset['sentence1'], dataset['sentence2'])]\n",
        "  score=[]\n",
        "  b_score=[]\n",
        "  for i in dataset['labels']:\n",
        "    score.append(i['label'])\n",
        "    b_score.append(i['binary-label'])\n",
        "  input_examples = pd.DataFrame({'guid': dataset['guid'], 'sent_pair' : sent_pairs, 'label' : score, 'bi': b_score})\n",
        "\n",
        "  return input_examples"
      ],
      "metadata": {
        "id": "wZHDQHfsdEdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sts_train_examples = make_input_ex_sts(train)\n",
        "sts_valid_examples = make_input_ex_sts(valid)\n",
        "sts_test_examples = make_input_ex_sts(test)"
      ],
      "metadata": {
        "id": "tFqOxSpQY4Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    - input_data: list of string\n",
        "    - target_data: list of int\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_data:list, target_data:list):\n",
        "        self.X = input_data\n",
        "        self.Y = target_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]"
      ],
      "metadata": {
        "id": "eNIDhmxLWNk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(sts_train_examples.sent_pair.to_list(), sts_train_examples.label.to_list())\n",
        "valid_dataset = CustomDataset(sts_valid_examples.sent_pair.to_list(), sts_valid_examples.label.to_list())"
      ],
      "metadata": {
        "id": "jLRZ-TWgWPag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWg6ua9paTiY",
        "outputId": "dca2d463-2e5c-43b2-8be9-c36f03932bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10501, 1167)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "q_fqWuBRapFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 접기 - 실패\n"
      ],
      "metadata": {
        "id": "3zsEP0AsXhW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def batch_encode(tokenizer, sent_pairs):\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "                              sent_pairs,\n",
        "                              max_length=512,\n",
        "                              padding=\"longest\",    \n",
        "                              add_special_tokens=True,\n",
        "                              truncation=True,\n",
        "                              return_tensors=\"pt\"\n",
        "                              )\n",
        "    return batch_encoding"
      ],
      "metadata": {
        "id": "_-oNvMrSi-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do encode\n",
        "transformers.logging.set_verbosity_error() # https://github.com/huggingface/transformers/issues/14285\n",
        "t_batch_encoding = batch_encode(tokenizer, sts_train_examples['sent_pair'])\n",
        "v_batch_encoding = batch_encode(tokenizer, sts_valid_examples['sent_pair']) \n",
        "d_batch_encoding = batch_encode(tokenizer, sts_test_examples['sent_pair'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ-RIVipoQTB",
        "outputId": "98337d36-5176-4258-ca10-7f5c862936f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10501/10501 [00:00<00:00, 503773.19it/s]\n",
            "100%|██████████| 1167/1167 [00:00<00:00, 714041.25it/s]\n",
            "100%|██████████| 519/519 [00:00<00:00, 769202.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs 수 확인\n",
        "print(\"\\t\\t   train\\tvalid\\t\\tdev\\n\")\n",
        "print(f\"input_ids        : {len(t_batch_encoding['input_ids'])}\\t{len(v_batch_encoding['input_ids'])}\\t\\t{len(d_batch_encoding['input_ids'])}\")\n",
        "print(f\"attention_mask   : {len(t_batch_encoding['attention_mask'])}\\t{len(v_batch_encoding['attention_mask'])}\\t\\t{len(d_batch_encoding['attention_mask'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRSGvW1HIqgv",
        "outputId": "35ab2c6c-9d3a-49a3-9e22-84be3e7a66f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t   train\tvalid\t\tdev\n",
            "\n",
            "input_ids        : 10501\t1167\t\t519\n",
            "attention_mask   : 10501\t1167\t\t519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 여기부터"
      ],
      "metadata": {
        "id": "LpJ_OzcfX5rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9zDqWP3hrWf",
        "outputId": "078f1b2f-cf7e-4d65-d762-a654b4812610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My current directory : /content/drive/MyDrive/AIBootCamp/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# argument setting\n",
        "task = \"sts\"\n",
        "model_checkpoint = \"roberta-base\" \n",
        "train_batch_size = 32\n",
        "eval_batch_size = 32\n",
        "epochs=3\n",
        "loss_fct = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "A2milq0OaqP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom collate_fn \n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    한 배치 내 문장들을 tokenizing 한 후 텐서로 변환함. \n",
        "    이때, dynamic padding (즉, 같은 배치 내 토큰의 개수가 동일할 수 있도록, 부족한 문장에 [PAD] 토큰을 추가하는 작업)을 적용\n",
        "    \n",
        "    한 배치 내 레이블(target)은 텐서화 함.\n",
        "    \n",
        "    - batch: list of tuples (input_data(string), target_data(int))\n",
        "    \"\"\"\n",
        "    input_list, target_list = [], []\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
        "    \n",
        "    for _input, _target in batch:\n",
        "        input_list.append(_input)\n",
        "        target_list.append(_target)\n",
        "    \n",
        "    tensorized_input = tokenizer(\n",
        "        input_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\", # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        truncation=True, # max_length를 넘는 문장은 이 후 토큰을 제거함\n",
        "        max_length=512,\n",
        "        return_tensors='pt' # 토크나이즈된 결과 값을 텐서 형태로 반환\n",
        "    )\n",
        "    \n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "    \n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "L3WSj4xZRxIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset,\n",
        "                             sampler = RandomSampler(train_dataset),\n",
        "                             batch_size=train_batch_size,\n",
        "                             collate_fn = custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset,\n",
        "                             sampler = RandomSampler(train_dataset),\n",
        "                             batch_size=eval_batch_size,\n",
        "                             collate_fn = custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "Ylac-b-3grxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int, n_label: int):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "        dropout_rate = 0.1\n",
        "        # linear_layer_hidden_size = 32\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "        # nn.Linear(hidden_size, linear_layer_hidden_size),\n",
        "        nn.Linear(hidden_size, n_label)\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        \n",
        "        # RoBERTa 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "        last_hidden_states = outputs[0] # last hidden states (batch_size, sequence_len, hidden_size)\n",
        "        cls_token_last_hidden_states = last_hidden_states[:,0,:] # (batch_size, first_token, hidden_size)\n",
        "\n",
        "        logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9NP-BiSsa4h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    \n",
        "    # model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint) # https://github.com/huggingface/transformers/issues/8879\n",
        "    model = CustomClassifier(hidden_size=128, n_label=5)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=2e-5,\n",
        "        eps=1e-8\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "ZtYezNeKJ71G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, scheduler = initializer(train_dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj2UbfWCOE1D",
        "outputId": "42940b6f-a228-4aef-969a-6e56d8f1b56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 3 epochs: 987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n",
        "    file_name = f'{path}/model.ckpt.{epoch}'\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "u0eQyuuQiZMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader):\n",
        "   \n",
        "    # 모델을 evaluate 모드로 설정 & device 할당\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_acc= 0,0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        # tensor 연산 전, 각 tensor에 device 할당\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch_input)\n",
        "            \n",
        "        # loss\n",
        "        loss = loss_fct(logits, batch_label)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # accuracy\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).flatten()\n",
        "        acc = (preds == batch_label).cpu().numpy().mean()\n",
        "        total_acc+=acc\n",
        "    \n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_acc = total_acc/(step+1)*100\n",
        "\n",
        "    return total_loss, total_acc"
      ],
      "metadata": {
        "id": "5SyffTTPNkgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, valid_dataloader=None, epochs=2):\n",
        "        \n",
        "        # train_dataloaer 학습을 epochs만큼 반복\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            \n",
        "            # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n",
        "            total_loss, batch_loss, batch_count = 0,0,0\n",
        "        \n",
        "            # model을 train 모드로 설정 & device 할당\n",
        "            model.train()\n",
        "            model.to(device)\n",
        "            \n",
        "            # data iterator를 돌면서 하나씩 학습\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_count+=1\n",
        "                \n",
        "                # tensor 연산 전, 각 tensor에 device 할당\n",
        "                batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "                batch_input, batch_label = batch\n",
        "            \n",
        "                # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "                model.zero_grad()\n",
        "            \n",
        "                # forward\n",
        "                logits = model(**batch_input)\n",
        "            \n",
        "                # loss\n",
        "                loss = loss_fct(logits, batch_label)\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "                # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "                loss.backward()\n",
        "                \n",
        "                # gradient clipping 적용 \n",
        "                clip_grad_norm_(model.parameters(), 1.0)\n",
        "                \n",
        "                # optimizer & scheduler 업데이트\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                \n",
        "                # 배치 10개씩 처리할 때마다 평균 loss와 lr를 출력\n",
        "                if (step % 10 == 0 and step != 0):\n",
        "                    learning_rate = optimizer.param_groups[0]['lr']\n",
        "                    print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "\n",
        "                    # reset \n",
        "                    batch_loss, batch_count = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "            \n",
        "            if valid_dataloader is not None:\n",
        "                print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "                valid_loss, valid_acc = validate(model, valid_dataloader)\n",
        "                print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid Acc : {valid_acc:.2f}\")\n",
        "                print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "            \n",
        "            # checkpoint 저장\n",
        "            save_checkpoint(\".\", model, optimizer, scheduler, epoch, total_loss/(step+1))\n",
        "                \n",
        "        print(\"Train Completed. End Program.\")"
      ],
      "metadata": {
        "id": "uf8Qlqz1N6B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "k-hBPUs8dAO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_dataloader, valid_dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "eHQjoGloio9n",
        "outputId": "cf7ff122-0fb8-4f50-822a-bb57c7ee3f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-0c2b6e54a806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-98ef06610828>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, valid_dataloader, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f393e21c445f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         )\n\u001b[1;32m    755\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         return super().forward(\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 14.76 GiB total capacity; 13.06 GiB already allocated; 19.75 MiB free; 13.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P25wEXrfWNQR",
        "outputId": "31f69e87-0d99-477f-9c74-b9b5e9197d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 26 08:18:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0    31W /  70W |  15068MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}